# significance of model trained on permuted data
classification_table[nrow(classification_table) + 1, ] <- c("permuted", -log10(perm_predict))
# Random part
random_feat <- as.data.frame(colnames(features)) %>%
sample_n(numFeatures, replace = FALSE)
names(random_feat)[1] = 'feature'
rand_feature_list <- as.vector(random_feat$feature)
# RANDOM linear model significance
if(numFeatures == 1) {
rand_idx <- match(rand_feature_list, colnames(trainData))
rand_subsetTrainData <- as.data.frame(trainData[,rand_idx])
rand_subsetTrainData <- cbind(trainData[,1], rand_subsetTrainData)
colnames(rand_subsetTrainData)[1] <- "Response"
colnames(rand_subsetTrainData)[2] <-rand_feature_list
} else {
rand_idx <- match(rand_feature_list, colnames(trainData))
rand_subsetTrainData <- trainData[,rand_idx]
rand_subsetTrainData <- cbind(trainData[,1], rand_subsetTrainData)
colnames(rand_subsetTrainData)[1] <- "Response"
}
rand_lmfit = lm(Response~., data = rand_subsetTrainData)
testData$rand_predicted_COH <- predict(rand_lmfit, newdata=testData) #predict total triglyceride of train data
rand_predict = cor.test(testData$rand_predicted_COH, testData$COH, method = 'pearson')$p.value
# significance of model trained on permuted data
classification_table[nrow(classification_table) + 1, ] <- c("random", -log10(rand_predict))
}
}
}
}
return(classification_table)
}
set.seed(123)
class_table <- runLassoRegression(lasso_input, 5, 5, 0, 20)
med_actual <- median(as.numeric(filter(class_table, condition == 'actual')$accuracy))
med_permute <- median(as.numeric(filter(class_table, condition == 'permuted')$accuracy))
med_random <- median(as.numeric(filter(class_table, condition == 'random')$accuracy))
View(totalCOH)
ggplot(aes(x = totalCOH)) + geom_density()
ggplot(lasso_input, aes(x = totalCOH)) + geom_density()
ggplot(totalCOH, aes(x = COH)) + geom_density()
ggplot(totalCE, aes(x = totalCE)) + geom_density()
ggplot(totalCOH, aes(x = COH)) + geom_density()
ggplot(totalCE, aes(x = totalCE)) + geom_density()
View(totalCE)
# run regression ========================================
runLassoRegression <- function(input_data, numFolds, numRep, numPerm, lambda_idx) {
# things to initialize =================================================================
classification_table <- data.frame("condition" = character(),
"accuracy" = double(),
stringsAsFactors=FALSE)
# get p value of overall model with this function
lmp <- function (modelobject) {
if (class(modelobject) != "lm") stop("Not an object of class 'lm' ")
f <- summary(modelobject)$fstatistic
p <- pf(f[1],f[2],f[3],lower.tail=F)
attributes(p) <- NULL
return(p)
}
# =======================================================================================
# Regression part
for(j in 1:numRep) {
folds <- cut(seq(1,nrow(input_data)), breaks=numFolds, labels=FALSE) %>% sample(., length(.), replace= F)
for(i in 1:numFolds) {
foldNum = i # to keep track of where we are
# Initialize folds, training and test datasets
testIdx <- which(folds == i, arr.ind=TRUE)
trainIdx <- which(folds != i, arr.ind= TRUE)
testData <-  input_data[testIdx,]
trainData <- input_data[-testIdx,]
features <- as.matrix(trainData[,-1])
class <- as.vector(trainData[,1])
# Perform cross-validation and extract betas from LASSO
numFeatures = 0
cvfit = cv.glmnet(features, class, alpha = 1, intercept = TRUE, grouped = FALSE)
betas = as.data.frame(as.matrix(coef(cvfit, s = cvfit$lambda[lambda_idx]))) %>%
tibble::rownames_to_column(var = 'feature')
names(betas) <- c('feature', 'beta')
betas <- betas %>% dplyr::filter(beta != 0, feature != '(Intercept)')
feature_list <- as.vector(betas$feature)
print(feature_list)
numFeatures <- length(feature_list)
print(paste('Number features selected: ', numFeatures))
# linear model significance
if(numFeatures == 1) {
idx <- match(feature_list, colnames(trainData))
subsetTrainData <- as.data.frame(trainData[,idx])
subsetTrainData <- cbind(trainData[,1], subsetTrainData)
colnames(subsetTrainData)[1] <- "Response"
colnames(subsetTrainData)[2] <- feature_list
} else {
idx <- match(feature_list, colnames(trainData))
subsetTrainData <- trainData[,idx]
subsetTrainData <- cbind(trainData[,1], subsetTrainData)
colnames(subsetTrainData)[1] <- "Response"
}
lmfit = lm(Response~., data = subsetTrainData)
testData$predicted_COH <- predict(lmfit, newdata=testData) #predict total triglyceride of train data
actual_predict = cor.test(testData$predicted_COH, testData$COH, method = 'pearson')$p.value
# significance of model trained on actual data
classification_table[nrow(classification_table) + 1, ] <- c("actual", -log10(actual_predict))
# PERMUTED AND RANDOM DATA =====================================================================
print(paste('Round', foldNum,'/5 in permutation testing for repetition', j,'.'))
if(numPerm > 0) {
for(repetition in 1:numPerm) {
perm_class = sample(class)
perm_numFeatures = 0
# internal check to make sure the response vector is not constant
perm_cvfit = cv.glmnet(features, perm_class, alpha = 1, intercept = TRUE)
all_betas = as.data.frame(as.matrix(coef(perm_cvfit, s = perm_cvfit$lambda[lambda_idx]))) %>%
tibble::rownames_to_column(var = 'feature')
names(all_betas) <- c('feature', 'beta')
subset_betas <- all_betas %>% filter(beta != 0, feature != '(Intercept)')
perm_feature_list <- as.vector(subset_betas$feature)
perm_numFeatures <- length(perm_feature_list)
if(perm_numFeatures < 1) {
rand_betas <- all_betas %>% filter(feature != '(Intercept)') %>%
sample_n(2, replace = FALSE)
perm_feature_list <- as.vector(rand_betas$feature)
perm_numFeatures <- length(perm_feature_list)
}
# PERMUTED linear model significance
if(perm_numFeatures == 1) {
perm_idx <- match(perm_feature_list, colnames(trainData))
perm_subsetTrainData <- as.data.frame(trainData[,perm_idx])
perm_subsetTrainData <- cbind(trainData[,1], perm_subsetTrainData)
colnames(perm_subsetTrainData)[1] <- "Response"
colnames(perm_subsetTrainData)[2] <- perm_feature_list
} else {
perm_idx <- match(perm_feature_list, colnames(trainData))
perm_subsetTrainData <- trainData[,perm_idx]
perm_subsetTrainData <- cbind(trainData[,1], perm_subsetTrainData)
colnames(perm_subsetTrainData)[1] <- "Response"
}
perm_lmfit = lm(Response~., data = perm_subsetTrainData)
testData$perm_predicted_COH <- predict(perm_lmfit, newdata=testData) #predict total triglyceride of train data
perm_predict = cor.test(testData$perm_predicted_COH, testData$COH, method = 'pearson')$p.value
# significance of model trained on permuted data
classification_table[nrow(classification_table) + 1, ] <- c("permuted", -log10(perm_predict))
# Random part
random_feat <- as.data.frame(colnames(features)) %>%
sample_n(numFeatures, replace = FALSE)
names(random_feat)[1] = 'feature'
rand_feature_list <- as.vector(random_feat$feature)
# RANDOM linear model significance
if(numFeatures == 1) {
rand_idx <- match(rand_feature_list, colnames(trainData))
rand_subsetTrainData <- as.data.frame(trainData[,rand_idx])
rand_subsetTrainData <- cbind(trainData[,1], rand_subsetTrainData)
colnames(rand_subsetTrainData)[1] <- "Response"
colnames(rand_subsetTrainData)[2] <-rand_feature_list
} else {
rand_idx <- match(rand_feature_list, colnames(trainData))
rand_subsetTrainData <- trainData[,rand_idx]
rand_subsetTrainData <- cbind(trainData[,1], rand_subsetTrainData)
colnames(rand_subsetTrainData)[1] <- "Response"
}
rand_lmfit = lm(Response~., data = rand_subsetTrainData)
testData$rand_predicted_COH <- predict(rand_lmfit, newdata=testData) #predict total triglyceride of train data
rand_predict = cor.test(testData$rand_predicted_COH, testData$COH, method = 'pearson')$p.value
# significance of model trained on permuted data
classification_table[nrow(classification_table) + 1, ] <- c("random", -log10(rand_predict))
}
}
}
}
return(classification_table)
}
set.seed(123)
class_table <- runLassoRegression(lasso_input, 5, 5, 0, 5)
med_actual <- median(as.numeric(filter(class_table, condition == 'actual')$accuracy))
hmdp_lipid = read.table('~/Desktop/HMDP/raw_data/new_data/HMDP_liver_lipidomics_allstrains_individual_lipid_species_for_TV_plus_identifiers.txt', sep = '\t', header = FALSE, stringsAsFactors = FALSE)
View(hmdp_lipid)
#lipid_input = as.data.frame(t(hmdp_lipid[,-1]))
lipid_input = setnames(lipid_input, old = colnames(lipid_input)[1:313], new = as.character(lipid_names$V1))
View(lipid_input)
#lipid_input = as.data.frame(t(hmdp_lipid[,-1]))
lipid_input = setnames(hmdp_lipid, old = colnames(hmdp_lipid)[1:313], new = as.character(lipid_names$V1))
View(lipid_input)
#lipid_input = as.data.frame(t(hmdp_lipid[,-1]))
lipid_input = setnames(hmdp_lipid[-1,], old = colnames(hmdp_lipid)[1:313], new = as.character(lipid_names$V1))
View(lipid_input)
totalCOH = lipid_input %>% select(1, starts_with("COH"))
lasso_input = left_join(totalCOH, protein_input, by = 'Mouse_ID') %>%
select(-Mouse_ID, -Strain) %>% data.frame(stringsAsFactors = FALSE)
ggplot(totalCOH, aes(x = COH)) + geom_density()
View(totalCOH)
ggplot(totalCOH, aes(x = COH)) + geom_density()
warnings()
hmdp_lipid = read.table('~/Desktop/HMDP/raw_data/new_data/HMDP_liver_lipidomics_allstrains_individual_lipid_species_for_TV_plus_identifiers.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE)
lipid_input = hmdp_lipid
totalCOH = lipid_input %>% select(1, starts_with("COH"))
lasso_input = left_join(totalCOH, protein_input, by = 'Mouse_ID') %>%
select(-Mouse_ID, -Strain) %>% data.frame(stringsAsFactors = FALSE)
ggplot(totalCE, aes(x = totalCE)) + geom_density()
ggplot(totalCOH, aes(x = COH)) + geom_density()
ggplot(totalCE, aes(x = totalCE)) + geom_density()
ggplot(totalCOH, aes(x = COH)) + geom_density()
# run regression ========================================
runLassoRegression <- function(input_data, numFolds, numRep, numPerm, lambda_idx) {
# things to initialize =================================================================
classification_table <- data.frame("condition" = character(),
"accuracy" = double(),
stringsAsFactors=FALSE)
# get p value of overall model with this function
lmp <- function (modelobject) {
if (class(modelobject) != "lm") stop("Not an object of class 'lm' ")
f <- summary(modelobject)$fstatistic
p <- pf(f[1],f[2],f[3],lower.tail=F)
attributes(p) <- NULL
return(p)
}
# =======================================================================================
# Regression part
for(j in 1:numRep) {
folds <- cut(seq(1,nrow(input_data)), breaks=numFolds, labels=FALSE) %>% sample(., length(.), replace= F)
for(i in 1:numFolds) {
foldNum = i # to keep track of where we are
# Initialize folds, training and test datasets
testIdx <- which(folds == i, arr.ind=TRUE)
trainIdx <- which(folds != i, arr.ind= TRUE)
testData <-  input_data[testIdx,]
trainData <- input_data[-testIdx,]
features <- as.matrix(trainData[,-1])
class <- as.vector(trainData[,1])
# Perform cross-validation and extract betas from LASSO
numFeatures = 0
cvfit = cv.glmnet(features, class, alpha = 1, intercept = TRUE, grouped = FALSE)
betas = as.data.frame(as.matrix(coef(cvfit, s = cvfit$lambda[lambda_idx]))) %>%
tibble::rownames_to_column(var = 'feature')
names(betas) <- c('feature', 'beta')
betas <- betas %>% dplyr::filter(beta != 0, feature != '(Intercept)')
feature_list <- as.vector(betas$feature)
print(feature_list)
numFeatures <- length(feature_list)
print(paste('Number features selected: ', numFeatures))
# linear model significance
if(numFeatures == 1) {
idx <- match(feature_list, colnames(trainData))
subsetTrainData <- as.data.frame(trainData[,idx])
subsetTrainData <- cbind(trainData[,1], subsetTrainData)
colnames(subsetTrainData)[1] <- "Response"
colnames(subsetTrainData)[2] <- feature_list
} else {
idx <- match(feature_list, colnames(trainData))
subsetTrainData <- trainData[,idx]
subsetTrainData <- cbind(trainData[,1], subsetTrainData)
colnames(subsetTrainData)[1] <- "Response"
}
lmfit = lm(Response~., data = subsetTrainData)
testData$predicted_COH <- predict(lmfit, newdata=testData) #predict total triglyceride of train data
actual_predict = cor.test(testData$predicted_COH, testData$COH, method = 'pearson')$p.value
# significance of model trained on actual data
classification_table[nrow(classification_table) + 1, ] <- c("actual", -log10(actual_predict))
# PERMUTED AND RANDOM DATA =====================================================================
print(paste('Round', foldNum,'/5 in permutation testing for repetition', j,'.'))
if(numPerm > 0) {
for(repetition in 1:numPerm) {
perm_class = sample(class)
perm_numFeatures = 0
# internal check to make sure the response vector is not constant
perm_cvfit = cv.glmnet(features, perm_class, alpha = 1, intercept = TRUE)
all_betas = as.data.frame(as.matrix(coef(perm_cvfit, s = perm_cvfit$lambda[lambda_idx]))) %>%
tibble::rownames_to_column(var = 'feature')
names(all_betas) <- c('feature', 'beta')
subset_betas <- all_betas %>% filter(beta != 0, feature != '(Intercept)')
perm_feature_list <- as.vector(subset_betas$feature)
perm_numFeatures <- length(perm_feature_list)
if(perm_numFeatures < 1) {
rand_betas <- all_betas %>% filter(feature != '(Intercept)') %>%
sample_n(2, replace = FALSE)
perm_feature_list <- as.vector(rand_betas$feature)
perm_numFeatures <- length(perm_feature_list)
}
# PERMUTED linear model significance
if(perm_numFeatures == 1) {
perm_idx <- match(perm_feature_list, colnames(trainData))
perm_subsetTrainData <- as.data.frame(trainData[,perm_idx])
perm_subsetTrainData <- cbind(trainData[,1], perm_subsetTrainData)
colnames(perm_subsetTrainData)[1] <- "Response"
colnames(perm_subsetTrainData)[2] <- perm_feature_list
} else {
perm_idx <- match(perm_feature_list, colnames(trainData))
perm_subsetTrainData <- trainData[,perm_idx]
perm_subsetTrainData <- cbind(trainData[,1], perm_subsetTrainData)
colnames(perm_subsetTrainData)[1] <- "Response"
}
perm_lmfit = lm(Response~., data = perm_subsetTrainData)
testData$perm_predicted_COH <- predict(perm_lmfit, newdata=testData) #predict total triglyceride of train data
perm_predict = cor.test(testData$perm_predicted_COH, testData$COH, method = 'pearson')$p.value
# significance of model trained on permuted data
classification_table[nrow(classification_table) + 1, ] <- c("permuted", -log10(perm_predict))
# Random part
random_feat <- as.data.frame(colnames(features)) %>%
sample_n(numFeatures, replace = FALSE)
names(random_feat)[1] = 'feature'
rand_feature_list <- as.vector(random_feat$feature)
# RANDOM linear model significance
if(numFeatures == 1) {
rand_idx <- match(rand_feature_list, colnames(trainData))
rand_subsetTrainData <- as.data.frame(trainData[,rand_idx])
rand_subsetTrainData <- cbind(trainData[,1], rand_subsetTrainData)
colnames(rand_subsetTrainData)[1] <- "Response"
colnames(rand_subsetTrainData)[2] <-rand_feature_list
} else {
rand_idx <- match(rand_feature_list, colnames(trainData))
rand_subsetTrainData <- trainData[,rand_idx]
rand_subsetTrainData <- cbind(trainData[,1], rand_subsetTrainData)
colnames(rand_subsetTrainData)[1] <- "Response"
}
rand_lmfit = lm(Response~., data = rand_subsetTrainData)
testData$rand_predicted_COH <- predict(rand_lmfit, newdata=testData) #predict total triglyceride of train data
rand_predict = cor.test(testData$rand_predicted_COH, testData$COH, method = 'pearson')$p.value
# significance of model trained on permuted data
classification_table[nrow(classification_table) + 1, ] <- c("random", -log10(rand_predict))
}
}
}
}
return(classification_table)
}
set.seed(123)
class_table <- runLassoRegression(lasso_input, 5, 5, 0, 5)
med_actual <- median(as.numeric(filter(class_table, condition == 'actual')$accuracy))
View(class_table)
runFinalLassoModel <- function(input_data, lambda_idx) {
set.seed(123)
final_cv = cv.glmnet(as.matrix(input_data[,-1]), as.vector(input_data[,1]), nfolds = 5, alpha = 1, intercept = TRUE)
final_betas = as.data.frame(as.matrix(coef(final_cv, s = final_cv$lambda[lambda_idx]))) %>%
tibble::rownames_to_column(var = 'feature')
names(final_betas) <- c('feature', 'beta')
final_betas <- final_betas %>% filter(beta != 0, feature != '(Intercept)')
final_feature_list <- as.vector(final_betas$feature)
return(final_feature_list)
}
selectFeat <- runFinalLassoModel(lasso_input, 5)
# read data in ======================================
hmdp_protein = read.table('~/Desktop/HMDP/raw_data/new_data/proteomics.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE)
hmdp_lipid = read.table('~/Desktop/HMDP/raw_data/new_data/HMDP_liver_lipidomics_allstrains_individual_lipid_species_for_TV_plus_identifiers.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE)
protein_names = read.csv('~/Desktop/HMDP/raw_data/new_data/protein_mapping.csv', header = FALSE, stringsAsFactors = FALSE)
lipid_names = read.table('~/Desktop/HMDP/raw_data/new_data/lipid_colNames.txt', header = FALSE, sep = '\t', stringsAsFactors = FALSE)
# formatting data ======================================
hmdp_protein[hmdp_protein=="NaN"] <- NA
protein_input = hmdp_protein[ ,colSums(is.na(hmdp_protein)) <= 145] # remove any column with NA data, ALL replicates must have measurements
protein_input <- cbind(protein_input[,c(1,2)], as.data.frame(knn.impute(as.matrix(as.data.frame(lapply(protein_input[,-c(1,2)], as.numeric))))))
lipid_input = hmdp_lipid
totalCE = lipid_input %>% select(1, starts_with("CE ")) %>%
gather(Lipid, Value, 2:14) %>%
group_by(Mouse_ID) %>%
mutate(totalCE = sum(as.numeric(Value))) %>%
ungroup() %>%
select(-Lipid, -Value) %>%
distinct()
totalCE = lipid_input %>% select(1, starts_with("CE "))
View(totalCE)
# read data in ======================================
hmdp_protein = read.table('~/Desktop/HMDP/raw_data/new_data/proteomics.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE)
hmdp_lipid = read.table('~/Desktop/HMDP/raw_data/new_data/HMDP_liver_lipidomics_allstrains_individual_lipid_species_for_TV_plus_identifiers.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE)
protein_names = read.csv('~/Desktop/HMDP/raw_data/new_data/protein_mapping.csv', header = FALSE, stringsAsFactors = FALSE)
lipid_names = read.table('~/Desktop/HMDP/raw_data/new_data/lipid_colNames.txt', header = FALSE, sep = '\t', stringsAsFactors = FALSE)
lipid_input = hmdp_lipid
View(lipid_input)
totalCE = lipid_input %>% select(1, starts_with("CE."))
totalCE = lipid_input %>% select(1, starts_with("CE.")) %>%
gather(Lipid, Value, 2:14) %>%
group_by(Mouse_ID) %>%
mutate(totalCE = sum(as.numeric(Value))) %>%
ungroup() %>%
select(-Lipid, -Value) %>%
distinct()
lasso_input = left_join(totalCE, protein_input, by = 'Mouse_ID') %>%
select(-Mouse_ID, -Strain) %>% data.frame(stringsAsFactors = FALSE)
# run regression ========================================
runLassoRegression <- function(input_data, numFolds, numRep, numPerm, lambda_idx) {
# things to initialize =================================================================
classification_table <- data.frame("condition" = character(),
"accuracy" = double(),
stringsAsFactors=FALSE)
# get p value of overall model with this function
lmp <- function (modelobject) {
if (class(modelobject) != "lm") stop("Not an object of class 'lm' ")
f <- summary(modelobject)$fstatistic
p <- pf(f[1],f[2],f[3],lower.tail=F)
attributes(p) <- NULL
return(p)
}
# =======================================================================================
# Regression part
for(j in 1:numRep) {
folds <- cut(seq(1,nrow(input_data)), breaks=numFolds, labels=FALSE) %>% sample(., length(.), replace= F)
for(i in 1:numFolds) {
foldNum = i # to keep track of where we are
# Initialize folds, training and test datasets
testIdx <- which(folds == i, arr.ind=TRUE)
trainIdx <- which(folds != i, arr.ind= TRUE)
testData <-  input_data[testIdx,]
trainData <- input_data[-testIdx,]
features <- as.matrix(trainData[,-1])
class <- as.vector(trainData[,1])
# Perform cross-validation and extract betas from LASSO
numFeatures = 0
cvfit = cv.glmnet(features, class, alpha = 1, intercept = TRUE, grouped = FALSE)
betas = as.data.frame(as.matrix(coef(cvfit, s = cvfit$lambda[lambda_idx]))) %>%
tibble::rownames_to_column(var = 'feature')
names(betas) <- c('feature', 'beta')
betas <- betas %>% dplyr::filter(beta != 0, feature != '(Intercept)')
feature_list <- as.vector(betas$feature)
print(feature_list)
numFeatures <- length(feature_list)
print(paste('Number features selected: ', numFeatures))
# linear model significance
if(numFeatures == 1) {
idx <- match(feature_list, colnames(trainData))
subsetTrainData <- as.data.frame(trainData[,idx])
subsetTrainData <- cbind(trainData[,1], subsetTrainData)
colnames(subsetTrainData)[1] <- "Response"
colnames(subsetTrainData)[2] <- feature_list
} else {
idx <- match(feature_list, colnames(trainData))
subsetTrainData <- trainData[,idx]
subsetTrainData <- cbind(trainData[,1], subsetTrainData)
colnames(subsetTrainData)[1] <- "Response"
}
lmfit = lm(Response~., data = subsetTrainData)
testData$predicted_CE <- predict(lmfit, newdata=testData) #predict total triglyceride of train data
actual_predict = cor.test(testData$predicted_CE, testData$totalCE, method = 'pearson')$p.value
# significance of model trained on actual data
classification_table[nrow(classification_table) + 1, ] <- c("actual", -log10(actual_predict))
# PERMUTED AND RANDOM DATA =====================================================================
print(paste('Round', foldNum,'/5 in permutation testing for repetition', j,'.'))
if(numPerm > 0) {
for(repetition in 1:numPerm) {
perm_class = sample(class)
perm_numFeatures = 0
# internal check to make sure the response vector is not constant
perm_cvfit = cv.glmnet(features, perm_class, alpha = 1, intercept = TRUE)
all_betas = as.data.frame(as.matrix(coef(perm_cvfit, s = perm_cvfit$lambda[lambda_idx]))) %>%
tibble::rownames_to_column(var = 'feature')
names(all_betas) <- c('feature', 'beta')
subset_betas <- all_betas %>% filter(beta != 0, feature != '(Intercept)')
perm_feature_list <- as.vector(subset_betas$feature)
perm_numFeatures <- length(perm_feature_list)
if(perm_numFeatures < 1) {
rand_betas <- all_betas %>% filter(feature != '(Intercept)') %>%
sample_n(1, replace = FALSE)
perm_feature_list <- as.vector(rand_betas$feature)
perm_numFeatures <- length(perm_feature_list)
}
# PERMUTED linear model significance
if(perm_numFeatures == 1) {
perm_idx <- match(perm_feature_list, colnames(trainData))
perm_subsetTrainData <- as.data.frame(trainData[,perm_idx])
perm_subsetTrainData <- cbind(trainData[,1], perm_subsetTrainData)
colnames(perm_subsetTrainData)[1] <- "Response"
colnames(perm_subsetTrainData)[2] <- perm_feature_list
} else {
perm_idx <- match(perm_feature_list, colnames(trainData))
perm_subsetTrainData <- trainData[,perm_idx]
perm_subsetTrainData <- cbind(trainData[,1], perm_subsetTrainData)
colnames(perm_subsetTrainData)[1] <- "Response"
}
perm_lmfit = lm(Response~., data = perm_subsetTrainData)
testData$perm_predicted_CE <- predict(perm_lmfit, newdata=testData) #predict total triglyceride of train data
perm_predict = cor.test(testData$perm_predicted_CE, testData$totalCE, method = 'pearson')$p.value
# significance of model trained on permuted data
classification_table[nrow(classification_table) + 1, ] <- c("permuted", -log10(perm_predict))
# Random part
random_feat <- as.data.frame(colnames(features)) %>%
sample_n(numFeatures, replace = FALSE)
names(random_feat)[1] = 'feature'
rand_feature_list <- as.vector(random_feat$feature)
# RANDOM linear model significance
if(numFeatures == 1) {
rand_idx <- match(rand_feature_list, colnames(trainData))
rand_subsetTrainData <- as.data.frame(trainData[,rand_idx])
rand_subsetTrainData <- cbind(trainData[,1], rand_subsetTrainData)
colnames(rand_subsetTrainData)[1] <- "Response"
colnames(rand_subsetTrainData)[2] <-rand_feature_list
} else {
rand_idx <- match(rand_feature_list, colnames(trainData))
rand_subsetTrainData <- trainData[,rand_idx]
rand_subsetTrainData <- cbind(trainData[,1], rand_subsetTrainData)
colnames(rand_subsetTrainData)[1] <- "Response"
}
rand_lmfit = lm(Response~., data = rand_subsetTrainData)
testData$rand_predicted_CE <- predict(rand_lmfit, newdata=testData) #predict total triglyceride of train data
rand_predict = cor.test(testData$rand_predicted_CE, testData$totalCE, method = 'pearson')$p.value
# significance of model trained on permuted data
classification_table[nrow(classification_table) + 1, ] <- c("random", -log10(rand_predict))
}
}
}
}
return(classification_table)
}
set.seed(123)
class_table <- runLassoRegression(lasso_input, 5, 10, 1, 4)
class_table <- runLassoRegression(lasso_input, 5, 10, 1, 10)
set.seed(123)
class_table <- runLassoRegression(lasso_input, 5, 10, 1, 10)
runFinalLassoModel <- function(input_data, lambda_idx) {
set.seed(123)
final_cv = cv.glmnet(as.matrix(input_data[,-1]), as.vector(input_data[,1]), nfolds = 5, alpha = 1, intercept = TRUE)
final_betas = as.data.frame(as.matrix(coef(final_cv, s = final_cv$lambda[lambda_idx]))) %>%
tibble::rownames_to_column(var = 'feature')
names(final_betas) <- c('feature', 'beta')
final_betas <- final_betas %>% filter(beta != 0, feature != '(Intercept)')
final_feature_list <- as.vector(final_betas$feature)
return(final_feature_list)
}
selectFeat <- runFinalLassoModel(lasso_input, 10)
select_proteins = protein_names %>% filter(V1 %in% selectFeat)
View(select_proteins)
